#!/usr/bin/env python3
"""CLI for code vector database operations"""

import sys
import os
import argparse
import json
import hashlib
from pathlib import Path

# Add library to path
sys.path.insert(0, os.path.expanduser('~/.local/lib'))

from code_vector_db.indexer import CodebaseIndexer
from code_vector_db.query import QueryInterface
from code_vector_db.metadata import ProjectMetadata


def cmd_init(args):
    """Initialize vector database for project and index codebase"""
    indexer = CodebaseIndexer(args.project_path)
    indexer.initialize()
    print(f"\n✓ Initialized vector database for: {args.project_path}")
    print("\nIndexing codebase...")
    indexer.index_codebase(incremental=False)


def cmd_index(args):
    """Index the codebase"""
    indexer = CodebaseIndexer(args.project_path)
    repo_filter = getattr(args, 'repo', None)
    indexer.index_codebase(incremental=args.incremental, repo_filter=repo_filter)


def cmd_reindex_file(args):
    """Reindex a single file"""
    indexer = CodebaseIndexer(args.project_path)
    indexer.reindex_file(args.file)
    print(f"✓ Reindexed: {args.file}")


def _read_code_snippet(project_path, file_path, start_line, end_line, context_lines=3, max_lines=50):
    """Read code snippet from file with optional context lines"""
    try:
        # In workspace mode, file_path already includes the repo subdirectory
        # e.g., "builder/processors/build.js" or "cms/include/sendProgress.js"
        full_path = Path(project_path) / file_path

        if not full_path.exists():
            return None

        with open(full_path, 'r', errors='ignore') as f:
            lines = f.readlines()

        # Add context lines
        start = max(0, start_line - 1 - context_lines)
        end = min(len(lines), end_line + context_lines)

        snippet_lines = lines[start:end]

        # Truncate if too long
        if len(snippet_lines) > max_lines:
            kept_start = max_lines - 10
            omitted = len(snippet_lines) - max_lines
            snippet_lines = (
                snippet_lines[:kept_start] +
                [f"   ... ({omitted} lines omitted) ...\n"] +
                snippet_lines[-10:]
            )

        # Add line numbers and indent
        result = []
        for i, line in enumerate(snippet_lines):
            if '... (' in line and 'omitted' in line:
                result.append(line)
            else:
                actual_line = start + i + 1
                result.append(f"   {line.rstrip()}")

        return ''.join(result)
    except Exception:
        return None


def _format_search_result(result, index, args, base_path=None):
    """Format a single search result for display"""
    repo_prefix = f"[{result.metadata.get('repo', '')}] " if result.metadata.get('repo') else ""
    line_range = f"{result.start_line}-{result.end_line}" if result.end_line > result.start_line else str(result.start_line)
    lang_suffix = f" [{result.language}]" if result.language else ""

    print(f"{index}. [{result.score:.3f}] {repo_prefix}{result.file_path}:{line_range}{lang_suffix}")

    if result.name:
        print(f"   {result.type}: {result.name}")
    elif result.type:
        print(f"   type: {result.type}")

    if hasattr(args, 'show_parent') and args.show_parent and result.parent:
        print(f"   parent: {result.parent}")

    if hasattr(args, 'show_content') and args.show_content:
        if base_path is None:
            base_path = getattr(args, 'workspace_path', args.project_path)
        snippet = _read_code_snippet(
            base_path,
            result.file_path,
            result.start_line,
            result.end_line,
            context_lines=getattr(args, 'context_lines', 3)
        )
        if snippet:
            print(snippet)
        print()


def cmd_search_hybrid(args):
    """Hybrid search combining semantic + BM25 keyword matching"""
    query_interface = QueryInterface(args.project_path)

    results = query_interface.search_hybrid(
        args.query,
        limit=args.limit,
        threshold=args.threshold,
        bm25_weight=args.bm25_weight,
        semantic_weight=args.semantic_weight
    )

    if not results:
        print(f"\nNo results found for: '{args.query}'")
        print(f"Threshold: {args.threshold}")
        print("Try lowering threshold with -t 0.1 or -t 0.0 for more results")
        return

    weights_info = f"(semantic: {args.semantic_weight:.1f}, keyword: {args.bm25_weight:.1f})"
    print(f"\nFound {len(results)} results {weights_info}:\n")
    for i, result in enumerate(results, 1):
        _format_search_result(result, i, args)


def cmd_search(args):
    """Search code"""
    query_interface = QueryInterface(args.project_path)

    results = query_interface.search_code(
        args.query,
        limit=args.limit,
        threshold=args.threshold
    )

    if not results:
        print(f"\nNo results found for: '{args.query}'")
        print(f"Threshold: {args.threshold}")
        print("Try lowering threshold with -t 0.1 or -t 0.0 for more results")
        return

    print(f"\nFound {len(results)} results (threshold: {args.threshold}):\n")
    for i, result in enumerate(results, 1):
        _format_search_result(result, i, args)


def cmd_similar(args):
    """Find similar code - accepts file path or semantic query"""
    query_interface = QueryInterface(args.project_path)

    results = query_interface.find_similar(
        args.query,
        limit=args.limit,
        threshold=args.threshold
    )

    # Detect if it was a file path
    from pathlib import Path
    is_file = Path(args.query).exists() and Path(args.query).is_file()
    query_type = "file" if is_file else "query"

    if not results:
        print(f"\nNo similar code found for {query_type}: '{args.query}'")
        print(f"Threshold: {args.threshold}")
        print("Try lowering threshold with -t 0.3 or -t 0.0 for more results")
        return

    print(f"\nSimilar to {query_type}: '{args.query}'")
    print(f"Found {len(results)} results (threshold: {args.threshold}):\n")
    for i, result in enumerate(results, 1):
        _format_search_result(result, i, args)


def cmd_context(args):
    """Get context for a task"""
    query_interface = QueryInterface(args.project_path)

    context_files = query_interface.get_context_for_task(
        args.task,
        max_files=args.limit,
        threshold=args.threshold
    )

    if not context_files:
        print(f"\nNo relevant context found for: '{args.task}'")
        print(f"Threshold: {args.threshold}")
        print("Try lowering threshold with -t 0.2 or -t 0.0 for more results")
        return

    if args.json:
        print(json.dumps(context_files, indent=2))
    else:
        print(f"\nRelevant files for: '{args.task}' (threshold: {args.threshold})\n")
        for i, file_info in enumerate(context_files, 1):
            print(f"{i}. [{file_info['score']:.3f}] {file_info['file_path']}")
            print(f"   Reason: {file_info['reason']}")
            if file_info['lines']:
                print(f"   Lines: {file_info['lines']}")
            print()


def cmd_impact(args):
    """Analyze impact - accepts file path or semantic query"""
    query_interface = QueryInterface(args.project_path)

    results = query_interface.analyze_impact(
        args.query,
        depth=2,
        threshold=args.threshold
    )

    query_type = results.get("query_type", "unknown")
    print(f"\nImpact analysis for {query_type}: '{args.query}'")
    print(f"Threshold: {args.threshold}\n")

    if results["direct"]:
        print(f"Direct impacts ({len(results['direct'])}):")
        for result in results["direct"][:10]:
            print(f"  [{result.score:.3f}] {result.file_path}:{result.start_line}")
            if result.name:
                print(f"    {result.type}: {result.name}")
        print()
    else:
        print(f"No direct impacts found (threshold: {args.threshold})")
        print("Try lowering threshold with -t 0.3 or -t 0.0 for more results\n")

    if results["indirect"]:
        print(f"Indirect impacts ({len(results['indirect'])}):")
        for result in results["indirect"][:10]:
            print(f"  [{result.score:.3f}] {result.file_path}:{result.start_line}")
            if result.name:
                print(f"    {result.type}: {result.name}")
    elif results["direct"]:
        print(f"No indirect impacts found (threshold: {args.threshold})")


def cmd_search_docs(args):
    """Search documentation"""
    query_interface = QueryInterface(args.project_path)

    results = query_interface.search_documentation(
        args.query,
        limit=args.limit,
        threshold=args.threshold
    )

    if not results:
        print("No documentation found")
        return

    print(f"\nFound {len(results)} documentation results:\n")
    for i, result in enumerate(results, 1):
        print(f"{i}. [{result.score:.3f}] {result.file_path}")
        print()


def cmd_search_conversations(args):
    """Search conversation history"""
    query_interface = QueryInterface(args.project_path)

    results = query_interface.search_conversations(
        args.query,
        limit=args.limit,
        threshold=args.threshold
    )

    if not results:
        print("No conversations found")
        return

    print(f"\nFound {len(results)} conversation results:\n")
    for i, result in enumerate(results, 1):
        print(f"{i}. [{result.score:.3f}] Session: {result.metadata.get('session_id', 'unknown')[:12]}")
        print(f"   {result.metadata.get('timestamp', '')}")
        if result.content:
            print(f"   {result.content[:200]}...")
        print()


def cmd_stats(args):
    """Show statistics"""
    query_interface = QueryInterface(args.project_path)
    stats = query_interface.get_stats()

    print(f"\nVector Database Statistics")
    print(f"Project ID: {stats['project_id']}\n")
    print("Collections:")

    total_points = 0
    for collection, info in stats["collections"].items():
        count = info["points_count"]
        total_points += count
        print(f"  {collection:20s}: {count:6d} points")

    print(f"\nTotal: {total_points} points")


def cmd_install_hook(args):
    """Install git post-commit hook"""
    import shutil

    git_dir = Path(args.project_path) / ".git"
    if not git_dir.exists():
        print("Error: Not a git repository")
        return

    hook_source = Path.home() / ".local/share/code-vector-db/post-commit-hook"
    hook_dest = git_dir / "hooks" / "post-commit"

    # Backup existing hook if present
    if hook_dest.exists():
        backup = hook_dest.with_suffix(".backup")
        shutil.copy(hook_dest, backup)
        print(f"✓ Backed up existing hook to {backup}")

    # Install hook
    shutil.copy(hook_source, hook_dest)
    hook_dest.chmod(0o755)

    print(f"✓ Installed post-commit hook at {hook_dest}")
    print("  Vector database will auto-update on commits")


def cmd_index_git(args):
    """Index git commit history into vector database"""
    from code_vector_db.embeddings import get_text_embedder
    from code_vector_db.vector_store import VectorStore
    from code_vector_db.metadata import ProjectMetadata
    import subprocess

    vector_store = VectorStore(args.project_path)
    text_embedder = get_text_embedder()
    metadata = ProjectMetadata()

    # Check if incremental mode
    incremental = getattr(args, 'incremental', False)

    # Check if path is a git repo or workspace
    project_path = Path(args.project_path)
    git_repos = []

    # Check if it's a direct git repo
    if (project_path / ".git").exists():
        git_repos.append(("", project_path))
    else:
        # Check for multi-repo workspace
        for item in project_path.iterdir():
            if item.is_dir() and (item / ".git").exists():
                git_repos.append((item.name, item))

    if not git_repos:
        print("No git repositories found")
        return

    mode_str = " (incremental)" if incremental else ""
    print(f"Found {len(git_repos)} git repositories{mode_str}", flush=True)

    # First pass: count commits per repo
    print("\nAnalyzing repositories...", flush=True)
    repo_commit_counts = []
    for repo_name, repo_path in git_repos:
        try:
            count_cmd = [
                "git", "-C", str(repo_path), "rev-list",
                "--count", "--no-merges", "HEAD"
            ]
            count_result = subprocess.run(count_cmd, capture_output=True, text=True, check=True)
            total_available = int(count_result.stdout.strip())

            if hasattr(args, 'limit') and args.limit:
                to_index = min(total_available, args.limit)
                repo_commit_counts.append((repo_name or "main", total_available, to_index))
                print(f"  {repo_name or 'main'}: {to_index:,} commits to index ({total_available:,} total)", flush=True)
            else:
                to_index = total_available
                repo_commit_counts.append((repo_name or "main", total_available, to_index))
                print(f"  {repo_name or 'main'}: {to_index:,} commits to index", flush=True)
        except subprocess.CalledProcessError:
            continue

    total_commits = 0
    points = []

    for repo_name, repo_path in git_repos:
        repo_label = f" [{repo_name}]" if repo_name else ""
        print(f"\nIndexing git history{repo_label}...", flush=True)

        try:
            # Build git log command
            cmd = [
                "git", "-C", str(repo_path), "log",
                "--format=%H|%an|%ae|%at|%s",
                "--no-merges"
            ]

            # Check for incremental mode
            if incremental:
                last_commit = metadata.get_last_indexed_commit(str(project_path), repo_name or "")
                if last_commit:
                    # Get commits since last indexed commit
                    cmd.extend([f"{last_commit}..HEAD"])
                    print(f"  Incremental: indexing commits since {last_commit[:8]}", flush=True)
                else:
                    # No previous index - index all commits
                    if hasattr(args, 'limit') and args.limit:
                        cmd.extend(["-n", str(args.limit)])
                        print(f"  No previous index found, indexing last {args.limit} commits", flush=True)
                    else:
                        print(f"  No previous index found, indexing all commits", flush=True)
            else:
                # Full index - index all commits (or use limit if specified)
                if hasattr(args, 'limit') and args.limit:
                    cmd.extend(["-n", str(args.limit)])
                    print(f"  Indexing last {args.limit} commits", flush=True)
                else:
                    print(f"  Indexing all commits", flush=True)

            result = subprocess.run(cmd, capture_output=True, text=True, check=True)

            # If no new commits in incremental mode
            if incremental and not result.stdout.strip():
                print(f"  No new commits since last index", flush=True)
                continue

            latest_commit_hash = None
            commit_count = 0

            # Parse all commits first
            commits_data = []
            for line in result.stdout.strip().split('\n'):
                if not line:
                    continue

                parts = line.split('|', 4)
                if len(parts) != 5:
                    continue

                commit_hash, author, email, timestamp, subject = parts

                # Track the first (latest) commit
                if latest_commit_hash is None:
                    latest_commit_hash = commit_hash

                commits_data.append({
                    'commit_hash': commit_hash,
                    'author': author,
                    'email': email,
                    'timestamp': timestamp,
                    'subject': subject
                })

            # Batch embed all commits at once (much faster!)
            if commits_data:
                texts_to_embed = [
                    f"{c['subject']}\n\nAuthor: {c['author']}\nCommit: {c['commit_hash'][:8]}"
                    for c in commits_data
                ]

                print(f"  Generating embeddings for {len(commits_data)} commits...", flush=True)
                embeddings = text_embedder.embed(texts_to_embed)

                # Create points with embeddings
                for i, commit_data in enumerate(commits_data):
                    commit_hash = commit_data['commit_hash']
                    commit_id = hashlib.md5(commit_hash.encode()).hexdigest()

                    points.append({
                        "id": commit_id,
                        "vector": embeddings[i],
                        "metadata": {
                            "file_path": f"git:{commit_hash[:8]}",
                            "type": "git_commit",
                            "role": "commit",
                            "content": commit_data['subject'][:500],
                            "session_id": commit_hash,
                            "timestamp": commit_data['timestamp'],
                            "model": commit_data['author'],
                            "start_line": 0,
                            "end_line": 0,
                            "name": commit_hash[:8],
                            "parent": "",
                            "language": "",
                            "content_hash": commit_hash,
                            "repo": repo_name
                        }
                    })

                    total_commits += 1
                    commit_count += 1

                    # Batch insert every 100 commits
                    if len(points) >= 100:
                        print(f"  Indexed {total_commits} commits...", flush=True)
                        vector_store.upsert_points(VectorStore.GIT_HISTORY, points)
                        points = []

            # Store the latest commit hash for incremental updates
            if latest_commit_hash and commit_count > 0:
                metadata.set_last_indexed_commit(str(project_path), repo_name or "", latest_commit_hash)
                print(f"  ✓ Indexed {commit_count} commits (latest: {latest_commit_hash[:8]})", flush=True)

        except subprocess.CalledProcessError as e:
            print(f"  Error reading git history: {e}")

    # Insert remaining points
    if points:
        vector_store.upsert_points(VectorStore.GIT_HISTORY, points)

    print(f"\n✓ Indexed {total_commits} git commits", flush=True)
    print(f"  Use code-vector-cli search-git '<query>' to search commits", flush=True)


def cmd_search_git(args):
    """Search git commit history"""
    query_interface = QueryInterface(args.project_path)

    from code_vector_db.embeddings import get_text_embedder
    from code_vector_db.vector_store import VectorStore

    text_embedder = get_text_embedder()
    vector_store = VectorStore(args.project_path)

    query_vector = text_embedder.embed(args.query)[0]

    results = vector_store.search(
        collection=VectorStore.GIT_HISTORY,
        query_vector=query_vector,
        limit=args.limit,
        score_threshold=args.threshold
    )

    if not results:
        print(f"\nNo git commits found for: '{args.query}'")
        print(f"Threshold: {args.threshold}")
        print("Try lowering threshold with -t 0.1 or run: code-vector-cli index-git")
        return

    print(f"\nFound {len(results)} git commits (threshold: {args.threshold}):\n")
    for i, result in enumerate(results, 1):
        metadata = result["metadata"]
        repo_prefix = f"[{metadata.get('repo', '')}] " if metadata.get('repo') else ""

        print(f"{i}. [{result['score']:.3f}] {repo_prefix}{metadata.get('name', 'unknown')}")
        print(f"   {metadata.get('content', '')}")
        print(f"   Author: {metadata.get('model', 'unknown')}")
        print()


def cmd_migrate_conversations(args):
    """Migrate conversation transcripts to vector database"""
    from code_vector_db.embeddings import get_text_embedder
    from code_vector_db.vector_store import VectorStore
    import json

    transcript_dir = Path(args.project_path) / ".claude-transcripts"
    if not transcript_dir.exists():
        print(f"No transcript directory found at {transcript_dir}")
        return

    # Find all transcript files
    transcript_files = list(transcript_dir.glob("*.jsonl"))
    if not transcript_files:
        print("No transcript files found")
        return

    print(f"Found {len(transcript_files)} transcript files")

    vector_store = VectorStore(args.project_path)
    text_embedder = get_text_embedder()

    total_messages = 0
    points = []

    for transcript_file in transcript_files:
        try:
            with open(transcript_file) as f:
                for line in f:
                    if not line.strip():
                        continue

                    record = json.loads(line)

                    # Handle Claude Code transcript format
                    message = record.get("message", {})
                    role = message.get("role", "")
                    content = message.get("content", "")

                    if not content or role not in ["user", "assistant"]:
                        continue

                    # Extract text content (skip tool_use and tool_result blocks)
                    text_content = ""
                    if isinstance(content, str):
                        text_content = content
                    elif isinstance(content, list):
                        text_blocks = []
                        for block in content:
                            if isinstance(block, dict):
                                # Only process text blocks, skip tool_use and tool_result
                                if block.get("type") == "text" and "text" in block:
                                    text_blocks.append(block["text"])
                        text_content = " ".join(text_blocks)

                    if not text_content or len(text_content) < 10:
                        # Skip short messages silently
                        continue

                    # Generate embedding
                    embedding = text_embedder.embed(text_content)[0]

                    # Generate unique ID for this conversation message
                    message_id = hashlib.md5(
                        f"{transcript_file.stem}:{record.get('uuid', '')}:{total_messages}".encode()
                    ).hexdigest()

                    points.append({
                        "id": message_id,  # Unique ID per message
                        "vector": embedding,
                        "metadata": {
                            "file_path": str(transcript_file.name),
                            "type": "conversation",
                            "role": role,
                            "content": text_content[:500],  # Store snippet
                            "session_id": record.get("sessionId", transcript_file.stem),
                            "timestamp": record.get("timestamp", ""),
                            "model": message.get("model", ""),
                            "start_line": 0,
                            "end_line": 0,
                            "name": "",
                            "parent": "",
                            "language": "",
                            "content_hash": ""
                        }
                    })

                    total_messages += 1

                    # Batch insert every 100 messages
                    if len(points) >= 100:
                        print(f"  Upserting batch of {len(points)} messages...")
                        vector_store.upsert_points(VectorStore.CONVERSATIONS, points)
                        points = []
                        print(f"  ✓ Migrated {total_messages} messages total")

        except Exception as e:
            print(f"  Error processing {transcript_file}: {e}")

    # Insert remaining points
    if points:
        print(f"  Inserting final batch of {len(points)} messages...")
        vector_store.upsert_points(VectorStore.CONVERSATIONS, points)

    print(f"\n✓ Migrated {total_messages} conversation messages")
    print(f"  Use code-vector-cli search-conversations '<query>' to search")



def cmd_list_projects(args):
    """List all indexed projects"""
    from qdrant_client import QdrantClient
    from collections import defaultdict

    metadata = ProjectMetadata()

    # Get collections from Qdrant
    try:
        client = QdrantClient(host="localhost", port=6333)
        collections_response = client.get_collections()
        collections = [c.name for c in collections_response.collections]
    except Exception as e:
        print(f"Error connecting to Qdrant: {e}")
        print("Make sure Qdrant is running: systemctl --user start qdrant")
        return

    # Group collections by project ID
    project_collections = defaultdict(list)
    for collection_name in collections:
        if "_" in collection_name:
            project_id = collection_name.rsplit("_", 2)[0]
            project_collections[project_id].append(collection_name)

    # Get metadata for all projects
    registered_projects = {p["project_id"]: p for p in metadata.list_all_projects()}

    if not project_collections:
        print("\nNo indexed projects found.")
        print("\nRun from a project directory:")
        print("  /v-init-single      # For single repo")
        print("  /v-init-workspace   # For multi-repo workspace")
        return

    print(f"\n{'='*70}")
    print("INDEXED PROJECTS")
    print(f"{'='*70}\n")

    for project_id in sorted(project_collections.keys()):
        collections = project_collections[project_id]
        metadata_info = registered_projects.get(project_id)

        print(f"Project ID: {project_id}")

        if metadata_info:
            print(f"  Path: {metadata_info['path']}")
            path_exists = Path(metadata_info['path']).exists()
            if not path_exists:
                print(f"  Status: ⚠️  Path no longer exists")
            else:
                print(f"  Status: ✓ Active")

            print(f"  Indexed: {metadata_info.get('indexed_at', 'unknown')}")
            print(f"  Updated: {metadata_info.get('last_updated', 'unknown')}")
            print(f"  Files: {metadata_info.get('file_count', 'unknown')}")

            if metadata_info.get('collection_stats'):
                total_points = sum(metadata_info['collection_stats'].values())
                print(f"  Vectors: {total_points:,}")
        else:
            print(f"  Path: ⚠️  Unknown (not in metadata registry)")
            print(f"  Status: Orphaned - no metadata")

        print(f"  Collections: {len(collections)}")

        # Get point counts from Qdrant
        total_vectors = 0
        for collection_name in collections:
            try:
                info = client.get_collection(collection_name)
                count = info.points_count
                total_vectors += count
                if args.verbose:
                    coll_type = collection_name.split("_", 1)[1] if "_" in collection_name else collection_name
                    print(f"    - {coll_type}: {count:,} points")
            except:
                pass

        if not args.verbose and total_vectors > 0:
            print(f"  Total vectors: {total_vectors:,}")

        print()

    print(f"{'='*70}\n")
    print(f"Total projects: {len(project_collections)}")

    # Check for orphaned metadata
    orphaned_meta = []
    for project_id, info in registered_projects.items():
        if project_id not in project_collections:
            orphaned_meta.append((project_id, info))

    if orphaned_meta:
        print(f"\n⚠️  Found {len(orphaned_meta)} projects in metadata but not in Qdrant:")
        for project_id, info in orphaned_meta:
            print(f"  - {project_id}: {info['path']}")
        print("\nRun: code-vector-cli cleanup-metadata")


def cmd_cleanup_metadata(args):
    """Clean up metadata for projects that no longer exist"""
    metadata = ProjectMetadata()
    removed = metadata.cleanup_missing_projects()

    if removed:
        print(f"\n✓ Removed {len(removed)} missing projects:")
        for item in removed:
            print(f"  - {item}")
    else:
        print("\n✓ No missing projects found. All metadata is valid.")


def cmd_delete(args):
    """Delete all collections for the project"""
    from code_vector_db.vector_store import VectorStore

    vector_store = VectorStore(args.project_path)

    # Confirm deletion unless --force is used
    if not args.force:
        project_id = vector_store.project_id
        print(f"\n⚠️  WARNING: This will delete all indexed data for:")
        print(f"   Path: {args.project_path}")
        print(f"   Project ID: {project_id}")
        print(f"\nCollections to be deleted:")
        for collection in VectorStore.ALL_COLLECTIONS:
            print(f"   - {collection}")

        response = input("\nAre you sure you want to continue? (yes/no): ")
        if response.lower() not in ['yes', 'y']:
            print("\n✗ Deletion cancelled")
            return

    # Delete collections
    print(f"\nDeleting collections for: {args.project_path}")
    vector_store.delete_collections()

    # Clean up metadata
    metadata = ProjectMetadata()
    metadata.unregister_project(args.project_path)

    print(f"\n✓ Successfully deleted all data for project")


def main():
    parser = argparse.ArgumentParser(
        description="Code Vector Database CLI"
    )
    parser.add_argument(
        "--path",
        default=".",
        help="Directory to index/search (auto-detects single-repo or multi-repo structure)"
    )

    subparsers = parser.add_subparsers(dest="command", help="Command to run")

    # init command
    subparsers.add_parser("init", help="Initialize vector database")

    # index command
    index_parser = subparsers.add_parser("index", help="Index codebase")
    index_parser.add_argument("--incremental", action="store_true", help="Incremental indexing")
    index_parser.add_argument("--repo", type=str, help="Index only specific repo in multi-repo workspace (e.g., 'base', 'frontend')")

    # reindex-file command
    reindex_parser = subparsers.add_parser("reindex-file", help="Reindex single file")
    reindex_parser.add_argument("file", help="File to reindex")

    # search command
    search_parser = subparsers.add_parser("search", help="Search code")
    search_parser.add_argument("query", help="Search query")
    search_parser.add_argument("-n", "--limit", type=int, default=10, help="Number of results")
    search_parser.add_argument("-t", "--threshold", type=float, default=0.3, help="Score threshold")
    search_parser.add_argument("--show-parent", action="store_true", help="Show parent class/module")
    search_parser.add_argument("--show-content", action="store_true", help="Show code snippets")
    search_parser.add_argument("-C", "--context-lines", type=int, default=3, help="Context lines around code snippets (default: 3)")

    # hybrid search command
    hybrid_parser = subparsers.add_parser("search-hybrid", help="Hybrid search (semantic + keyword BM25)")
    hybrid_parser.add_argument("query", help="Search query")
    hybrid_parser.add_argument("-n", "--limit", type=int, default=10, help="Number of results")
    hybrid_parser.add_argument("-t", "--threshold", type=float, default=0.3, help="Score threshold")
    hybrid_parser.add_argument("--show-parent", action="store_true", help="Show parent class/module")
    hybrid_parser.add_argument("--show-content", action="store_true", help="Show code snippets")
    hybrid_parser.add_argument("-C", "--context-lines", type=int, default=3, help="Context lines around code snippets (default: 3)")
    hybrid_parser.add_argument("--bm25-weight", type=float, default=0.3, help="BM25 keyword weight (default: 0.3)")
    hybrid_parser.add_argument("--semantic-weight", type=float, default=0.7, help="Semantic similarity weight (default: 0.7)")

    # similar command
    similar_parser = subparsers.add_parser("similar", help="Find similar code")
    similar_parser.add_argument("query", help="File path OR semantic query")
    similar_parser.add_argument("-n", "--limit", type=int, default=10, help="Number of results")
    similar_parser.add_argument("-t", "--threshold", type=float, default=0.7, help="Score threshold")
    similar_parser.add_argument("--show-content", action="store_true", help="Show code snippets")
    similar_parser.add_argument("-C", "--context-lines", type=int, default=3, help="Context lines around code snippets (default: 3)")

    # context command
    context_parser = subparsers.add_parser("context", help="Get context for task")
    context_parser.add_argument("task", help="Task description")
    context_parser.add_argument("-n", "--limit", type=int, default=10, help="Max files")
    context_parser.add_argument("-t", "--threshold", type=float, default=0.4, help="Score threshold")
    context_parser.add_argument("--json", action="store_true", help="Output as JSON")

    # impact command
    impact_parser = subparsers.add_parser("impact", help="Analyze impact")
    impact_parser.add_argument("query", help="File path OR semantic query")
    impact_parser.add_argument("-t", "--threshold", type=float, default=0.6, help="Score threshold")

    # search-docs command
    docs_parser = subparsers.add_parser("search-docs", help="Search documentation")
    docs_parser.add_argument("query", help="Search query")
    docs_parser.add_argument("-n", "--limit", type=int, default=10, help="Number of results")
    docs_parser.add_argument("-t", "--threshold", type=float, default=0.3, help="Score threshold")

    # search-conversations command
    conv_parser = subparsers.add_parser("search-conversations", help="Search conversations")
    conv_parser.add_argument("query", help="Search query")
    conv_parser.add_argument("-n", "--limit", type=int, default=5, help="Number of results")
    conv_parser.add_argument("-t", "--threshold", type=float, default=0.3, help="Score threshold")

    # index-git command
    git_index_parser = subparsers.add_parser("index-git", help="Index git commit history")
    git_index_parser.add_argument("-n", "--limit", type=int, default=None, help="Max commits to index per repo (default: all commits)")
    git_index_parser.add_argument("--incremental", action="store_true", help="Only index new commits since last run")

    # search-git command
    git_search_parser = subparsers.add_parser("search-git", help="Search git commits")
    git_search_parser.add_argument("query", help="Search query")
    git_search_parser.add_argument("-n", "--limit", type=int, default=10, help="Number of results")
    git_search_parser.add_argument("-t", "--threshold", type=float, default=0.3, help="Score threshold")

    # stats command
    subparsers.add_parser("stats", help="Show statistics")

    # install-hook command
    subparsers.add_parser("install-hook", help="Install git post-commit hook")

    # migrate-conversations command
    subparsers.add_parser("migrate-conversations", help="Migrate conversation transcripts")

    # list-projects command
    list_parser = subparsers.add_parser("list-projects", help="List all indexed projects")
    list_parser.add_argument("-v", "--verbose", action="store_true", help="Show detailed collection stats")

    # cleanup-metadata command
    subparsers.add_parser("cleanup-metadata", help="Remove metadata for missing projects")

    # delete command
    delete_parser = subparsers.add_parser("delete", help="Delete all indexed data for project")
    delete_parser.add_argument("--force", action="store_true", help="Skip confirmation prompt")

    args = parser.parse_args()

    if not args.command:
        parser.print_help()
        return

    # Resolve path
    args.path = os.path.abspath(args.path)

    # Set legacy aliases for backward compatibility with existing code
    args.project_path = args.path
    args.workspace_path = args.path

    # Dispatch to command handler
    command_map = {
        "init": cmd_init,
        "index": cmd_index,
        "reindex-file": cmd_reindex_file,
        "search": cmd_search,
        "search-hybrid": cmd_search_hybrid,
        "similar": cmd_similar,
        "context": cmd_context,
        "impact": cmd_impact,
        "search-docs": cmd_search_docs,
        "search-conversations": cmd_search_conversations,
        "index-git": cmd_index_git,
        "search-git": cmd_search_git,
        "stats": cmd_stats,
        "install-hook": cmd_install_hook,
        "migrate-conversations": cmd_migrate_conversations,
        "list-projects": cmd_list_projects,
        "cleanup-metadata": cmd_cleanup_metadata,
        "delete": cmd_delete,
    }

    handler = command_map.get(args.command)
    if handler:
        try:
            handler(args)
        except KeyboardInterrupt:
            print("\n\nInterrupted")
            sys.exit(1)
        except Exception as e:
            print(f"\nError: {e}", file=sys.stderr)
            import traceback
            traceback.print_exc()
            sys.exit(1)


if __name__ == "__main__":
    main()
